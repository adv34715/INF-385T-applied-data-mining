{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Data Mining -- Homework3\n",
    "## name: Yimeng Zhao\n",
    "## UT EID: yz9428\n",
    "### (a) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk \n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import gensim # topic modeling\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instances: 3081\n"
     ]
    }
   ],
   "source": [
    "# Merge together and treat each new-line as a single document\n",
    "import os\n",
    "\n",
    "# Open a file\n",
    "dirpath = \"../../downloads/merged/\"\n",
    "dirs = os.listdir(dirpath)\n",
    "merged = []\n",
    "slice_index = [] #to store where the file ends, for example, the first file,democratic file,\n",
    "                            # is form 0 to 1807, so slick_index[0]=1808 \n",
    "\n",
    "for file in dirs:\n",
    "    fin = open(os.path.join(dirpath,file),'rU')\n",
    "    raw = fin.readlines()\n",
    "    #print (len(raw),type(raw))\n",
    "    for line in raw:\n",
    "        merged.append(line)\n",
    "    slice_index.append(len(merged))\n",
    "    fin.close()\n",
    "print('instances:',len(merged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "import string\n",
    "\n",
    "\n",
    "def tokenize_docs(doc_list):\n",
    "    tokenized_docs = []\n",
    "    count = 0 # count total words\n",
    "    \n",
    "    for doc in doc_list:\n",
    "        '''# lowercase the words\n",
    "        doc_lowercase = doc.lower()'''\n",
    "    \n",
    "        # now tokenize\n",
    "        #doc_tokens = nltk.tokenize.word_tokenize(doc_lowercase)\n",
    "        doc_tokens = gensim.utils.tokenize(doc,lowercase = True, deacc=True)\n",
    "    \n",
    "        # remove meaningless words\n",
    "        stop_list = stopwords.words('english')\n",
    "        stop_list.extend(string.punctuation)\n",
    "        stop_list.extend([\"''\",\"...\",\"mdash\",\"/i\",\"``\",\"would\",\"'m\",\"'s\",\"'re\",\"'ve\",\"'ll\",\"n't\",\n",
    "                          \"applause\",\"laughter\",\"crosstalk\",\"--\"])\n",
    "    \n",
    "        doc_tokens = [word for word in doc_tokens if not word in stop_list]\n",
    "        tokenized_docs.append(doc_tokens)\n",
    "        count = count+len(doc_tokens)\n",
    "        \n",
    "    print(\"after tokenizing and removing some meaningless words, there are \",count,\" words\")\n",
    "    return tokenized_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stem \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def stem_docs(tokenized_docs):\n",
    "    p_stemmer = PorterStemmer()\n",
    "    tokenized_stemmed = [[p_stemmer.stem(w) for w in doc] for \n",
    "                             doc in tokenized_docs]\n",
    "    \n",
    "    # create a 'dictionary' for tokenzied and stemmed docs\n",
    "    dictionary = corpora.Dictionary(tokenized_stemmed)\n",
    "    print(dictionary)\n",
    "    \n",
    "    corpus = [dictionary.doc2bow(text) for text in tokenized_stemmed]\n",
    "    \n",
    "    return dictionary,corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after tokenizing and removing some meaningless words, there are  54314  words\n",
      "Dictionary(4151 unique tokens: ['scar', 'gold', 'bag', 'incred', 'texa']...)\n"
     ]
    }
   ],
   "source": [
    "toke_merged = tokenize_docs(merged)\n",
    "dict_merged, corpus_merged = stem_docs(toke_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "3081 instances. After tokenizing and stemming(remove many meaningless words), there are 54314 words and 4151 unique tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5  topics:\n",
      "[(0, '0.014*isi + 0.011*need + 0.010*new + 0.010*countri + 0.009*presid + 0.009*state + 0.008*know + 0.008*make + 0.008*peopl + 0.007*well'), (1, '0.015*state + 0.010*one + 0.010*said + 0.010*talk + 0.009*want + 0.009*say + 0.009*unit + 0.009*clinton + 0.008*go + 0.008*hillari'), (2, '0.019*peopl + 0.014*think + 0.013*go + 0.013*countri + 0.012*presid + 0.012*need + 0.010*american + 0.009*work + 0.009*get + 0.008*know'), (3, '0.026*tax + 0.014*go + 0.012*right + 0.011*want + 0.011*pay + 0.011*let + 0.011*obama + 0.011*money + 0.010*barack + 0.010*countri'), (4, '0.025*peopl + 0.021*go + 0.013*know + 0.009*question + 0.009*get + 0.008*wall + 0.008*back + 0.008*let + 0.008*well + 0.007*countri')] \n",
      "\n",
      "10  topics:\n",
      "[(0, '0.025*presid + 0.024*clinton + 0.021*countri + 0.019*hillari + 0.018*peopl + 0.015*elect + 0.015*america + 0.013*want + 0.013*obama + 0.012*state'), (1, '0.018*peopl + 0.014*said + 0.012*good + 0.009*think + 0.009*get + 0.009*problem + 0.008*democrat + 0.008*come + 0.008*veteran + 0.008*countri'), (2, '0.019*question + 0.018*let + 0.017*peopl + 0.016*say + 0.013*marco + 0.013*know + 0.013*want + 0.013*washington + 0.012*first + 0.011*amnesti'), (3, '0.016*isi + 0.015*state + 0.014*one + 0.011*go + 0.010*unit + 0.010*presid + 0.010*talk + 0.010*let + 0.010*gun + 0.010*need'), (4, '0.027*peopl + 0.023*work + 0.020*go + 0.018*know + 0.014*get + 0.014*american + 0.013*wall + 0.009*think + 0.009*need + 0.009*let'), (5, '0.024*think + 0.017*go + 0.011*presid + 0.010*know + 0.010*isi + 0.010*get + 0.010*obama + 0.009*year + 0.008*militari + 0.008*need'), (6, '0.014*get + 0.011*bank + 0.010*offic + 0.010*deal + 0.009*big + 0.009*note + 0.009*attorney + 0.008*make + 0.007*power + 0.007*execut'), (7, '0.023*law + 0.016*go + 0.016*peopl + 0.015*enforc + 0.011*win + 0.011*well + 0.011*take + 0.010*american + 0.009*want + 0.009*immigr'), (8, '0.017*peopl + 0.015*go + 0.013*countri + 0.011*make + 0.009*health + 0.009*want + 0.008*one + 0.008*us + 0.008*work + 0.008*care'), (9, '0.048*tax + 0.018*back + 0.018*pay + 0.017*state + 0.016*go + 0.016*new + 0.015*jersey + 0.014*need + 0.013*money + 0.011*busi')] \n",
      "\n",
      "20  topics:\n",
      "[(0, '0.038*let + 0.021*done + 0.017*first + 0.014*fact + 0.014*go + 0.014*good + 0.013*way + 0.013*mani + 0.012*somebodi + 0.011*even'), (1, '0.033*new + 0.030*jersey + 0.029*work + 0.019*state + 0.015*like + 0.013*see + 0.011*well + 0.011*ask + 0.011*listen + 0.010*disast'), (2, '0.020*emin + 0.019*domain + 0.019*voter + 0.016*folk + 0.014*washington + 0.014*drive + 0.013*speak + 0.012*countri + 0.012*without + 0.012*knew'), (3, '0.057*think + 0.026*know + 0.024*well + 0.019*presid + 0.014*peopl + 0.013*import + 0.012*get + 0.011*make + 0.010*point + 0.010*year'), (4, '0.034*attack + 0.017*ted + 0.016*question + 0.014*reason + 0.014*go + 0.012*note + 0.011*say + 0.011*take + 0.009*let + 0.009*allow'), (5, '0.020*wall + 0.017*need + 0.017*street + 0.014*go + 0.012*america + 0.012*nation + 0.011*know + 0.011*bill + 0.010*economi + 0.009*peopl'), (6, '0.021*want + 0.019*peopl + 0.019*go + 0.016*border + 0.013*get + 0.013*make + 0.013*trump + 0.012*tell + 0.012*secur + 0.011*win'), (7, '0.021*isi + 0.019*need + 0.016*go + 0.015*world + 0.013*safe + 0.012*keep + 0.012*war + 0.012*us + 0.011*air + 0.011*militari'), (8, '0.038*peopl + 0.029*law + 0.025*immigr + 0.024*gun + 0.021*enforc + 0.020*american + 0.018*support + 0.014*illeg + 0.011*countri + 0.010*bring'), (9, '0.025*social + 0.023*secur + 0.023*look + 0.021*deal + 0.021*need + 0.020*leav + 0.020*countri + 0.020*north + 0.020*one + 0.020*korea'), (10, '0.042*go + 0.038*talk + 0.035*issu + 0.023*let + 0.018*well + 0.018*get + 0.017*think + 0.014*deal + 0.012*job + 0.012*respons'), (11, '0.055*state + 0.044*unit + 0.027*presid + 0.017*go + 0.014*peopl + 0.012*second + 0.012*america + 0.011*ye + 0.010*one + 0.010*amend'), (12, '0.047*fight + 0.042*isi + 0.022*ring + 0.022*bell + 0.017*amnesti + 0.016*assad + 0.014*chang + 0.012*defeat + 0.010*war + 0.010*cannot'), (13, '0.025*peopl + 0.021*colleg + 0.020*new + 0.015*talk + 0.014*percent + 0.014*york + 0.011*american + 0.011*need + 0.011*free + 0.011*actual'), (14, '0.026*clinton + 0.024*hillari + 0.020*obama + 0.018*know + 0.017*barack + 0.015*countri + 0.015*marco + 0.014*thing + 0.014*want + 0.012*peopl'), (15, '0.033*thank + 0.027*presid + 0.017*go + 0.017*win + 0.017*execut + 0.014*day + 0.013*much + 0.012*campaign + 0.012*order + 0.012*bad'), (16, '0.024*america + 0.023*presid + 0.022*elect + 0.022*want + 0.021*chang + 0.017*make + 0.015*life + 0.014*like + 0.013*peopl + 0.012*govern'), (17, '0.026*right + 0.022*peopl + 0.021*men + 0.017*take + 0.016*women + 0.013*hous + 0.012*white + 0.011*democrat + 0.010*man + 0.009*said'), (18, '0.054*tax + 0.023*countri + 0.018*go + 0.016*pay + 0.014*peopl + 0.013*make + 0.012*back + 0.012*plan + 0.011*want + 0.011*percent'), (19, '0.028*peopl + 0.015*go + 0.014*care + 0.014*health + 0.012*million + 0.012*one + 0.012*countri + 0.011*know + 0.011*work + 0.010*american')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def topic_model(corpus,dictionary,topics,topwords):\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus, num_topics=topics, \n",
    "                                            id2word = dictionary, passes=10)\n",
    "    print(lda_model.print_topics(num_topics=topics, num_words=topwords),'\\n')\n",
    "    \n",
    "\n",
    "topics = [5,10,20]\n",
    "topwords = 10\n",
    "for k in topics:\n",
    "    print(k,' topics:')\n",
    "    topic_model(corpus_merged,dict_merged,k,topwords)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k = 10 is the best of these three ks. The topics of k = 10 are more understandable than those of k = 5, for example, in k = 5, (0, '0.014*isi + 0.011*need + 0.010*new + 0.010*countri + 0.009*presid + 0.009*state + 0.008*know + 0.008*make + 0.008*peopl + 0.007*well'), it is not clear what it means: it may be related to ISIS but very blur; while in k = 10, (0, '0.025*presid + 0.024*clinton + 0.021*countri + 0.019*hillari + 0.018*peopl + 0.015*elect + 0.015*america + 0.013*want + 0.013*obama + 0.012*state'), it seems like about election or president. \n",
    "The topics of k = 10 are better than those of k = 20, because k = 20 seems to have more overlaps and some topic words doen't make any sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "democratic:\n",
      "after tokenizing and removing some meaningless words, there are  31058  words\n",
      "Dictionary(3126 unique tokens: ['scar', 'gold', 'incred', 'learn', 'took']...)\n",
      "\n",
      " republican:\n",
      "after tokenizing and removing some meaningless words, there are  23256  words\n",
      "Dictionary(2818 unique tokens: ['gold', 'incred', 'texa', 'learn', 'took']...)\n"
     ]
    }
   ],
   "source": [
    "print(\"democratic:\")\n",
    "toke_democ = tokenize_docs(merged[:slice_index[0]])\n",
    "dict_democ, corpus_democ = stem_docs(toke_democ)\n",
    "\n",
    "print(\"\\n\",\"republican:\")\n",
    "toke_repub = tokenize_docs(merged[slice_index[0]:])\n",
    "dict_repub, corpus_repub = stem_docs(toke_repub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5  topics:\n",
      "[(0, '0.016*think + 0.014*know + 0.013*well + 0.011*work + 0.010*go + 0.010*countri + 0.010*peopl + 0.007*want + 0.006*get + 0.006*senat'), (1, '0.014*peopl + 0.013*go + 0.011*colleg + 0.010*care + 0.010*make + 0.010*pay + 0.009*think + 0.009*health + 0.008*want + 0.008*way'), (2, '0.019*peopl + 0.016*gun + 0.014*state + 0.010*say + 0.009*right + 0.007*american + 0.007*vote + 0.007*america + 0.007*look + 0.006*one'), (3, '0.016*street + 0.015*let + 0.015*wall + 0.012*secretari + 0.010*go + 0.010*know + 0.009*talk + 0.008*bank + 0.008*state + 0.007*governor'), (4, '0.016*think + 0.015*need + 0.015*peopl + 0.011*get + 0.011*countri + 0.011*go + 0.009*make + 0.009*american + 0.009*got + 0.008*believ')] \n",
      "\n",
      "10  topics:\n",
      "[(0, '0.018*need + 0.014*work + 0.011*new + 0.009*togeth + 0.009*state + 0.009*countri + 0.008*thing + 0.008*think + 0.007*get + 0.007*well'), (1, '0.019*think + 0.015*peopl + 0.015*presid + 0.015*right + 0.011*know + 0.010*want + 0.010*state + 0.009*well + 0.008*need + 0.008*go'), (2, '0.023*let + 0.020*peopl + 0.019*know + 0.017*think + 0.011*state + 0.010*american + 0.010*us + 0.010*ye + 0.009*need + 0.008*talk'), (3, '0.023*peopl + 0.016*work + 0.012*well + 0.012*go + 0.011*think + 0.010*make + 0.010*need + 0.009*look + 0.008*presid + 0.008*got'), (4, '0.018*colleg + 0.015*countri + 0.012*make + 0.012*go + 0.010*peopl + 0.010*america + 0.010*think + 0.008*free + 0.008*major + 0.008*percent'), (5, '0.025*street + 0.023*wall + 0.016*say + 0.011*peopl + 0.011*believ + 0.010*secretari + 0.009*know + 0.009*countri + 0.007*clinton + 0.007*well'), (6, '0.025*think + 0.012*get + 0.012*know + 0.012*go + 0.010*american + 0.009*presid + 0.009*tri + 0.009*come + 0.008*peopl + 0.007*syria'), (7, '0.021*go + 0.014*care + 0.014*get + 0.012*peopl + 0.010*health + 0.009*look + 0.008*pay + 0.008*want + 0.007*think + 0.007*presid'), (8, '0.019*peopl + 0.016*work + 0.015*class + 0.014*middl + 0.012*rais + 0.012*countri + 0.012*wage + 0.011*famili + 0.010*campaign + 0.010*make'), (9, '0.016*think + 0.016*said + 0.013*countri + 0.013*secretari + 0.011*state + 0.010*thank + 0.010*war + 0.010*one + 0.009*isi + 0.009*muslim')] \n",
      "\n",
      "20  topics:\n",
      "[(0, '0.027*peopl + 0.019*work + 0.014*need + 0.013*countri + 0.013*go + 0.012*rais + 0.012*american + 0.011*make + 0.009*get + 0.009*tax'), (1, '0.016*said + 0.016*american + 0.015*agre + 0.013*isi + 0.011*one + 0.011*countri + 0.011*support + 0.010*take + 0.009*let + 0.009*presid'), (2, '0.024*overtalk + 0.014*peopl + 0.013*well + 0.012*talk + 0.012*one + 0.010*think + 0.010*republican + 0.010*need + 0.009*state + 0.009*work'), (3, '0.019*want + 0.014*peopl + 0.011*give + 0.010*thing + 0.010*use + 0.009*said + 0.008*respond + 0.008*go + 0.008*american + 0.008*one'), (4, '0.022*state + 0.020*one + 0.017*need + 0.014*know + 0.013*go + 0.012*make + 0.010*also + 0.008*unit + 0.008*new + 0.008*peopl'), (5, '0.012*peopl + 0.011*countri + 0.011*look + 0.011*gun + 0.011*need + 0.009*american + 0.009*secretari + 0.009*wage + 0.009*absolut + 0.008*immigr'), (6, '0.013*need + 0.012*year + 0.011*new + 0.011*make + 0.011*fight + 0.010*thing + 0.010*know + 0.010*actual + 0.009*state + 0.009*done'), (7, '0.026*care + 0.023*countri + 0.023*health + 0.022*right + 0.021*peopl + 0.014*think + 0.014*major + 0.010*get + 0.009*go + 0.009*afford'), (8, '0.018*peopl + 0.017*need + 0.012*get + 0.011*well + 0.011*believ + 0.011*health + 0.011*reform + 0.011*countri + 0.009*crimin + 0.008*issu'), (9, '0.012*think + 0.012*go + 0.011*need + 0.010*lot + 0.009*get + 0.009*new + 0.008*believ + 0.008*put + 0.008*work + 0.008*countri'), (10, '0.017*colleg + 0.014*go + 0.012*work + 0.011*peopl + 0.011*pay + 0.011*know + 0.011*get + 0.011*make + 0.010*free + 0.010*countri'), (11, '0.017*go + 0.012*us + 0.011*american + 0.011*new + 0.010*question + 0.009*economi + 0.008*back + 0.008*big + 0.008*street + 0.008*know'), (12, '0.022*cost + 0.022*ye + 0.016*state + 0.012*secretari + 0.011*insur + 0.011*care + 0.011*year + 0.010*privat + 0.010*go + 0.009*afford'), (13, '0.037*think + 0.019*vote + 0.016*peopl + 0.015*know + 0.014*senat + 0.011*well + 0.010*see + 0.010*got + 0.009*gun + 0.009*state'), (14, '0.018*think + 0.017*republican + 0.015*gun + 0.012*democrat + 0.012*hous + 0.012*need + 0.009*clinton + 0.009*year + 0.008*point + 0.008*bank'), (15, '0.023*let + 0.022*peopl + 0.021*go + 0.016*know + 0.013*want + 0.011*us + 0.010*like + 0.010*start + 0.009*first + 0.009*american'), (16, '0.045*street + 0.041*wall + 0.018*presid + 0.014*secretari + 0.014*peopl + 0.013*need + 0.010*clinton + 0.010*countri + 0.010*economi + 0.010*think'), (17, '0.015*well + 0.015*say + 0.013*go + 0.012*think + 0.012*look + 0.011*thank + 0.009*know + 0.008*bank + 0.007*break + 0.007*talk'), (18, '0.020*peopl + 0.019*campaign + 0.014*countri + 0.013*work + 0.011*polit + 0.011*million + 0.011*go + 0.011*think + 0.009*make + 0.009*believ'), (19, '0.023*get + 0.020*think + 0.011*work + 0.011*go + 0.010*presid + 0.008*senat + 0.008*want + 0.008*happen + 0.008*isi + 0.007*well')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Democratic topics\n",
    "topics = [5,10,20]\n",
    "for k in topics:\n",
    "    print(k,' topics:')\n",
    "    topic_model(corpus_democ,dict_democ,k,topwords),'\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5  topics:\n",
      "[(0, '0.023*go + 0.013*peopl + 0.012*presid + 0.010*obama + 0.010*countri + 0.009*fight + 0.008*barack + 0.008*america + 0.008*state + 0.008*isi'), (1, '0.017*peopl + 0.015*get + 0.014*want + 0.011*think + 0.011*go + 0.010*one + 0.009*say + 0.007*presid + 0.007*let + 0.007*us'), (2, '0.019*tax + 0.009*one + 0.008*peopl + 0.008*want + 0.008*year + 0.008*well + 0.008*thing + 0.007*know + 0.007*new + 0.006*go'), (3, '0.018*go + 0.016*know + 0.013*peopl + 0.011*countri + 0.010*presid + 0.009*need + 0.008*state + 0.008*work + 0.007*isi + 0.007*get'), (4, '0.018*countri + 0.016*peopl + 0.014*presid + 0.012*go + 0.009*know + 0.008*issu + 0.008*law + 0.008*year + 0.007*come + 0.007*first')] \n",
      "\n",
      "10  topics:\n",
      "[(0, '0.022*tax + 0.017*state + 0.016*want + 0.014*us + 0.012*america + 0.011*peopl + 0.011*unit + 0.009*busi + 0.009*countri + 0.009*presid'), (1, '0.021*presid + 0.014*like + 0.012*countri + 0.012*make + 0.011*elect + 0.011*american + 0.010*america + 0.010*need + 0.008*let + 0.007*get'), (2, '0.021*go + 0.015*peopl + 0.012*countri + 0.012*say + 0.010*said + 0.010*year + 0.009*china + 0.009*back + 0.009*new + 0.009*win'), (3, '0.012*say + 0.011*let + 0.011*well + 0.011*want + 0.010*isi + 0.010*answer + 0.008*actual + 0.008*question + 0.007*one + 0.007*qaida'), (4, '0.020*peopl + 0.018*go + 0.016*come + 0.013*thank + 0.010*state + 0.010*know + 0.009*countri + 0.009*one + 0.008*talk + 0.007*america'), (5, '0.023*know + 0.019*countri + 0.017*clinton + 0.017*hillari + 0.012*obama + 0.011*like + 0.010*barack + 0.010*well + 0.009*go + 0.008*one'), (6, '0.012*go + 0.012*one + 0.011*right + 0.011*peopl + 0.010*think + 0.010*know + 0.010*want + 0.010*thing + 0.010*fight + 0.009*get'), (7, '0.027*peopl + 0.025*go + 0.018*get + 0.018*presid + 0.012*law + 0.012*isi + 0.009*way + 0.009*countri + 0.008*enforc + 0.008*american'), (8, '0.015*go + 0.009*health + 0.008*need + 0.008*question + 0.008*got + 0.007*insur + 0.007*know + 0.007*first + 0.006*american + 0.006*take'), (9, '0.026*tax + 0.016*peopl + 0.008*new + 0.008*us + 0.008*go + 0.008*know + 0.008*work + 0.008*want + 0.007*plan + 0.007*think')] \n",
      "\n",
      "20  topics:\n",
      "[(0, '0.030*right + 0.016*life + 0.015*go + 0.015*peopl + 0.014*issu + 0.010*ok + 0.010*new + 0.009*year + 0.008*support + 0.007*win'), (1, '0.027*one + 0.013*countri + 0.012*like + 0.012*state + 0.012*tax + 0.011*peopl + 0.010*radic + 0.009*percent + 0.008*thing + 0.008*money'), (2, '0.035*problem + 0.030*law + 0.026*enforc + 0.021*go + 0.017*peopl + 0.016*countri + 0.013*secur + 0.012*immigr + 0.012*take + 0.012*solv'), (3, '0.024*peopl + 0.017*use + 0.015*want + 0.012*take + 0.012*new + 0.011*say + 0.010*know + 0.009*percent + 0.008*isi + 0.007*ye'), (4, '0.046*peopl + 0.017*go + 0.015*countri + 0.014*support + 0.012*american + 0.011*amnesti + 0.010*say + 0.010*illeg + 0.010*come + 0.008*legal'), (5, '0.017*want + 0.015*think + 0.015*go + 0.015*believ + 0.013*law + 0.013*well + 0.012*see + 0.012*presid + 0.011*make + 0.009*get'), (6, '0.038*go + 0.014*time + 0.013*peopl + 0.013*countri + 0.012*win + 0.010*get + 0.010*lot + 0.010*think + 0.009*presid + 0.009*take'), (7, '0.018*said + 0.017*countri + 0.014*donald + 0.011*come + 0.010*tell + 0.010*born + 0.010*bill + 0.010*legal + 0.010*support + 0.009*american'), (8, '0.040*tax + 0.013*state + 0.013*busi + 0.012*american + 0.011*flat + 0.011*point + 0.010*second + 0.010*unit + 0.009*plan + 0.008*well'), (9, '0.028*know + 0.016*peopl + 0.015*well + 0.011*great + 0.009*first + 0.009*trump + 0.008*guy + 0.007*think + 0.007*new + 0.007*life'), (10, '0.015*peopl + 0.011*yeah + 0.009*year + 0.009*someon + 0.008*singl + 0.008*get + 0.008*new + 0.008*jersey + 0.008*right + 0.008*tell'), (11, '0.020*peopl + 0.014*go + 0.013*let + 0.013*like + 0.012*get + 0.011*deal + 0.010*talk + 0.010*domain + 0.010*emin + 0.009*want'), (12, '0.028*hillari + 0.025*clinton + 0.020*countri + 0.020*isi + 0.018*need + 0.013*obama + 0.012*presid + 0.011*defeat + 0.011*go + 0.011*command'), (13, '0.014*gun + 0.011*go + 0.011*know + 0.010*good + 0.010*take + 0.009*get + 0.008*china + 0.008*countri + 0.008*job + 0.008*well'), (14, '0.039*presid + 0.030*america + 0.028*state + 0.024*unit + 0.020*world + 0.019*militari + 0.016*want + 0.016*go + 0.015*elect + 0.014*barack'), (15, '0.018*peopl + 0.015*know + 0.014*happen + 0.012*go + 0.010*tax + 0.010*us + 0.010*let + 0.010*compani + 0.010*want + 0.009*get'), (16, '0.019*go + 0.019*presid + 0.015*know + 0.015*countri + 0.014*let + 0.014*say + 0.013*think + 0.012*first + 0.012*question + 0.012*like'), (17, '0.017*go + 0.016*say + 0.014*thank + 0.012*said + 0.011*secur + 0.011*fact + 0.011*differ + 0.010*want + 0.010*save + 0.010*clinton'), (18, '0.019*need + 0.017*get + 0.015*new + 0.014*look + 0.012*talk + 0.012*say + 0.012*peopl + 0.011*year + 0.011*go + 0.010*ted'), (19, '0.025*go + 0.016*peopl + 0.014*need + 0.012*tax + 0.011*one + 0.011*back + 0.010*done + 0.009*said + 0.009*get + 0.009*bring')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Republician topics\n",
    "topics = [5,10,20]\n",
    "for k in topics:\n",
    "    print(k,' topics:')\n",
    "    topic_model(corpus_repub,dict_repub,k,topwords),'\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In Democratic file, they might talk about sequrity(ISIS), work, health care, gun, tax, campaign, Republician, Hillary Clinton. Republicians talk about ISIS, election, China(compaign), tax, law or legality, immigration, military, Hillary Clinton. They both has some topics which less appear in merged file. Furthermore, there are some differences between these two parties. For example, in Democratic's speech, ISIS is often related with countries, United States, etc.; while related with fight, president in Republician's. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
